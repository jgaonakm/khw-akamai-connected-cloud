# Bootstraping Control Plane (part 2)

In this lab we continue setting up the control plane components by installing and configuring the Kubernetes binaries. As in the previous step, SSH into each control plane instance individually or simultaneously using `tmux`. 

```sh
ssh -i ~/.ssh/[key] root@[node]
```

> ðŸ’¡ The commands need to be executed in all three control plane instances


## Binaries

Download the Kubernetes server binaries; for this tutorial we're using v1.29.0, the latest as of time of writing.

https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG

```sh
wget -q --show-progress --https-only --timestamping \
https://dl.k8s.io/v1.29.0/kubernetes-server-linux-amd64.tar.gz
```

Extract and install them

```sh
tar -xvf kubernetes-server-linux-amd64.tar.gz 
cd kubernetes/server/bin
ls -l kube-apiserver kube-controller-manager kube-scheduler kubectl
chmod -v +x  kube-apiserver kube-controller-manager kube-scheduler kubectl
sudo mv -v kube-apiserver kube-controller-manager kube-scheduler kubectl /usr/local/bin/ 
```

## Configure the Kubernetes API Server

Move certificates and config files we generated earlier

```sh
cd ~
sudo mkdir -pv /var/lib/kubernetes/ 

sudo mv -v ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \
    service-account-key.pem service-account.pem \
    encryption-config.yaml /var/lib/kubernetes/
```

The instance internal IP address will be used to advertise the API Server to members of the cluster. Retrieve the internal IP address for the current compute instance using the metadata API:

```sh
TOKEN=$(curl -X PUT -H "Metadata-Token-Expiry-Seconds: 3600" http://169.254.169.254/v1/token)
IPAM=$(curl -H "Metadata-Token: $TOKEN" http://169.254.169.254/v1/network | grep interfaces.ipam_address | cut -d ' ' -f 2)
INTERNAL_IP=${IPAM::-3}
```

We also need the cluster public IP address, in this case from the NodeBalancer we created earlier. 
```sh
KUBERNETES_PUBLIC_ADDRESS=XXX.XXX.XXX.XXX
```

With this values we can create the service unit file:

```sh
cat <<EOF | sudo tee /etc/systemd/system/kube-apiserver.service
[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-apiserver \\
  --advertise-address=${INTERNAL_IP} \\
  --allow-privileged=true \\
  --apiserver-count=3 \\
  --audit-log-maxage=30 \\
  --audit-log-maxbackup=3 \\
  --audit-log-maxsize=100 \\
  --audit-log-path=/var/log/audit.log \\
  --authorization-mode=Node,RBAC \\
  --bind-address=0.0.0.0 \\
  --client-ca-file=/var/lib/kubernetes/ca.pem \\
  --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\
  --etcd-cafile=/var/lib/kubernetes/ca.pem \\
  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\
  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\
  --etcd-servers=https://10.240.0.10:2379,https://10.240.0.11:2379,https://10.240.0.12:2379 \\
  --event-ttl=1h \\
  --encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\
  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\
  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\
  --kubelet-preferred-address-types=InternalIP,ExternalIP \\
  --runtime-config='api/all=true' \\
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\
  --service-account-signing-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-account-issuer=https://${KUBERNETES_PUBLIC_ADDRESS}:6443 \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --service-node-port-range=30000-32767 \\
  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\
  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

> ðŸ’¡ Note that we set the [etcd-servers option](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/) with the list of internal IP addresses and ports we configured on the [previous lab](4.controlPlane-1.md). 

## Configure the Kubernetes Controller Manager

Move config file
```sh
  sudo mv -v kube-controller-manager.kubeconfig /var/lib/kubernetes/
```

And then create service unit file

```sh
cat <<EOF | sudo tee /etc/systemd/system/kube-controller-manager.service
[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-controller-manager \\
  --bind-address=0.0.0.0 \\
  --cluster-cidr=10.200.0.0/16 \\
  --cluster-name=kubernetes \\
  --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\
  --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\
  --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\
  --leader-elect=true \\
  --root-ca-file=/var/lib/kubernetes/ca.pem \\
  --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\
  --service-cluster-ip-range=10.32.0.0/24 \\
  --use-service-account-credentials=true \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

## Configure the Kubernetes Scheduler

Move config file:
```sh
sudo mkdir -p /etc/kubernetes/config
sudo mv -v kube-scheduler.kubeconfig /var/lib/kubernetes/
```

Create yaml configuration file:

```sh
cat <<EOF | sudo tee /etc/kubernetes/config/kube-scheduler.yaml
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: "/var/lib/kubernetes/kube-scheduler.kubeconfig"
leaderElection:
  leaderElect: true
EOF
```

Create the service unit file:

```sh
cat <<EOF | sudo tee /etc/systemd/system/kube-scheduler.service
[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-scheduler \\
  --config=/etc/kubernetes/config/kube-scheduler.yaml \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```


## Start the services

```sh
sudo systemctl daemon-reload
sudo systemctl enable kube-apiserver kube-controller-manager kube-scheduler
sudo systemctl start kube-apiserver kube-controller-manager kube-scheduler 
```

## Verify locally

Wait for around ten seconds, and validate the Control Plane is alive by running the healtchecks

```sh
curl -k https://127.0.0.1:6443/livez?verbose
curl -k https://127.0.0.1:6443/readyz?verbose
```

We can also test using kubectl

```sh
kubectl cluster-info --kubeconfig admin.kubeconfig
```

If both tests succeed, we can continue setting up the RBAC Authorization.

## RBAC Authorization

Here we create the permissions to allow the Kubernetes API Server to access the Kubelet API on each worker node. This is required for retrieving metrics, logs, and executing commands in pods.

First we create the `system:kube-apiserver-to-kubelet` [ClusterRole](https://kubernetes.io/docs/reference/access-authn-authz/rbac/#role-and-clusterrole) with permissions to access the Kubelet API and perform most common tasks associated with managing pods:

> These commands are to be **executed in a single instance**; they take effect in the entire cluster.

```sh
cat <<EOF | kubectl apply --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:kube-apiserver-to-kubelet
rules:
  - apiGroups:
      - ""
    resources:
      - nodes/proxy
      - nodes/stats
      - nodes/log
      - nodes/spec
      - nodes/metrics
    verbs:
      - "*"
EOF
```
Result:

> clusterrole.rbac.authorization.k8s.io/system:kube-apiserver-to-kubelet created


Then we bind the `system:kube-apiserver-to-kubelet` ClusterRole to the `kubernetes` user:
```sh
cat <<EOF | kubectl apply  --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: system:kube-apiserver
  namespace: ""
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:kube-apiserver-to-kubelet
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: kubernetes
EOF
```

Result:

> clusterrolebinding.rbac.authorization.k8s.io/system:kube-apiserver created


## Test from outside

Finally, from our local environment we call the API using the public IP address:


```sh
curl --cacert ca.pem https://${KUBERNETES_PUBLIC_ADDRESS}:6443/version
```

If everything is working correctly, the API will respond with the cluster information: 

```json
{
  "major": "1",
  "minor": "29",
  "gitVersion": "v1.29.0",
  "gitCommit": "3f7a50f38688eb332e2a1b013678c6435d539ae6",
  "gitTreeState": "clean",
  "buildDate": "2023-12-13T08:45:03Z",
  "goVersion": "go1.21.5",
  "compiler": "gc",
  "platform": "linux/amd64"
}%
```

---

Next > [Bootstrap worker nodes](5.workerNodes.md)


