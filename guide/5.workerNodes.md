# Bootstrap Worker Nodes

In this lab we bootstrap the worker nodes by installing and configuring the Kubernetes node binaries, as well as the container runtime and the CNI plugins. 

SSH into the worker nodes individually or at the same time using tmux

```sh
ssh -i ~/.ssh/[key] root@[node]
```

## Install dependencies

First, install the required dependencies:

```sh
sudo apt-get update
sudo apt-get -y install socat conntrack ipset
```

> `socat` enables support for the kubectl port-forward command.
> `conntrack` is required by kube-proxy to run

## Disable swap

By default the kubelet will fail to start if [swap](https://help.ubuntu.com/community/SwapFaq) is enabled. It is [recommended](https://github.com/kubernetes/kubernetes/issues/7294) that swap be disabled to ensure Kubernetes can provide proper resource allocation and quality of service.

Check if swap is enabled
```sh
sudo swapon --show
```

If output is empthy then swap is not enabled. If swap is enabled run the following command to disable swap immediately:

```sh 
sudo swapoff -av
```

To ensure is remains disabled, open the `/etc/fstab` file and comment out the swap line.

> ðŸ’¡ This process may be different if you choose a different distro for your nodes. Refer to the distro docs to get the right procedure.

## Download and install binaries

We'll use the following binaries:
1. [runC](https://www.docker.com/blog/runc/): lightweight, portable container runtime. It's used by containerd as the runtime engine. [(binaries)](https://github.com/opencontainers/runc/releases/)
1. [containerd](https://containerd.io): industry-standard container runtime with an emphasis on simplicity, robustness, and portability. It is available as a daemon for Linux and Windows, which can manage the complete container lifecycle of its host system: image transfer and storage, container execution and supervision, low-level storage and network attachments, etc. [(binaries)](https://github.com/containerd/containerd/releases)
1. [CNI plugins](https://github.com/containernetworking/plugins#plugins): CNI (Container Network Interface), a Cloud Native Computing Foundation project, consists of a specification and libraries for writing plugins to configure network interfaces in Linux and Windows containers, along with a number of supported plugins. [(binaries)](https://github.com/containernetworking/plugins/releases/)
1. K8s Node binaries: Components required in the node (kubelet & kube-proxy). [(binaries)](https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG)



Download the files
```sh
wget -q --show-progress --https-only --timestamping \
https://github.com/opencontainers/runc/releases/download/v1.1.10/runc.amd64 \
https://github.com/containernetworking/plugins/releases/download/v1.4.0/cni-plugins-linux-amd64-v1.4.0.tgz \
https://github.com/containerd/containerd/releases/download/v1.7.11/containerd-1.7.11-linux-amd64.tar.gz \
https://dl.k8s.io/v1.29.0/kubernetes-node-linux-amd64.tar.gz
```

Create installation directories
```sh
sudo mkdir -pv \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes
```

Install the binaries

```sh

sudo mv -v runc.amd64 runc
chmod +x -v runc 
sudo mv -v runc /usr/local/bin/

sudo tar -xvf cni-plugins-linux-amd64-v1.4.0.tgz -C /opt/cni/bin/

mkdir containerd
tar -xvf containerd-1.7.11-linux-amd64.tar.gz -C containerd
sudo mv -v containerd/bin/* /bin/

tar -xvf kubernetes-node-linux-amd64.tar.gz
cd kubernetes/node/bin/
chmod +x -v kubectl kube-proxy kubelet  
sudo mv -v kubectl kube-proxy kubelet /usr/local/bin/
cd ~
```


## Configure CNI Networking

The following plugin configuration allows the Kubernetes networking to work in our cluster. 

We'll start by defining a POD CIDR, out of the cluster range (10.200.0.0/16) we defined earlier. It needs to be different for each node, since pods will get IP's within the CIDR range of the node they reside. Replace the X with 0,1, or 2, aligning with the node nomenclature.

```sh
POD_CIDR=10.200.X.0/24
```

> ðŸ’¡ An environment variable is used to be able to execute the command below simultaneously for the three nodes using `tmux`

Create the configuration file for the bridge, which is used to connect pods to each other eth0 interface.

```sh
cat <<EOF | tee /etc/cni/net.d/10-containerd-net.conflist
{
  "cniVersion": "0.4.0",
  "name": "containerd-net",
  "plugins": [
    {
      "type": "bridge",
      "bridge": "br0",
      "isGateway": true,
      "ipMasq": true,
      "promiscMode": true,
      "ipam": {
        "type": "host-local",
        "ranges": [
          [{"subnet": "${POD_CIDR}"}]
        ],
        "routes": [
          { "dst": "0.0.0.0/0" },
          { "dst": "::/0" }
        ]
      }
    },
    {
      "type": "portmap",
      "capabilities": {"portMappings": true}
    }
  ]
}
EOF
```

Now we create the loopback configuration file. The `lo` interface allows all containers to communicate within the pod using `localhost`.

```sh
cat <<EOF | sudo tee /etc/cni/net.d/99-loopback.conf
{
    "cniVersion": "0.4.0",
    "name": "lo",
    "type": "loopback"
}
EOF
```


## Configure containerd

Create the configuration file

```sh
sudo mkdir -vp /etc/containerd/

cat << EOF | sudo tee /etc/containerd/config.toml 
version = 2

# persistent data location
root = "/var/lib/containerd"
# runtime state information
state = "/run/containerd"
# indicates how likely is the process to be terminated in case of low available memory
oom_score = 0

[grpc]
  address = "/run/containerd/containerd.sock"
  uid = 0
  gid = 0

[debug]
  address = "/run/containerd/debug.sock"
  uid = 0
  gid = 0
  level = "info"

[metrics]
  address = ""
  grpc_histogram = false

[cgroup]
  path = ""

[plugins]
  [plugins."io.containerd.monitor.v1.cgroups"]
    no_prometheus = false
  [plugins."io.containerd.service.v1.diff-service"]
    default = ["walking"]
  [plugins."io.containerd.gc.v1.scheduler"]
    pause_threshold = 0.02
    deletion_threshold = 0
    mutation_threshold = 100
    schedule_delay = 0
    startup_delay = "100ms"
  [plugins."io.containerd.runtime.v2.task"]
    platforms = ["linux/amd64"]
  [plugins."io.containerd.snapshotter.v1.overlayfs"]
    root_path = ""
  [plugins."io.containerd.internal.v1.restart"]
    interval = "10s"
  [plugins."io.containerd.grpc.v1.cri".containerd]
      default_runtime_name = "runc"
      disable_snapshot_annotations = true
      discard_unpacked_layers = false
      no_pivot = false
      snapshotter = "overlayfs"
EOF
```

Create the systemd unit file

```sh
cat <<EOF | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target local-fs.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd

Type=notify
Delegate=yes
KillMode=process
Restart=always
RestartSec=5

LimitNPROC=infinity
LimitCORE=infinity

TasksMax=infinity
OOMScoreAdjust=-999

[Install]
WantedBy=multi-user.target
EOF
```

## Configure the Kubelet

Move the certificates and config files

```sh
HOSTNAME=$(hostname -f)
sudo mv -v ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
sudo mv -v ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
sudo mv -v ca.pem /var/lib/kubernetes/
```

Create the configuration file

```sh
cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.32.0.10"
resolvConf: "/run/systemd/resolve/resolv.conf"
containerRuntimeEndpoint: "unix:///var/run/containerd/containerd.sock"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
EOF
```

> ðŸ’¡ The Cluster DNS service will be started later, using the IP provided here

> ðŸ’¡ The resolvConf configuration is used to avoid loops when using CoreDNS for service discovery on systems running systemd-resolved.

Create the systemd unit file

```sh
cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

## Configure the Kubernetes Proxy

Move the configuration file we generated earlier
```sh
sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
```

Create the configuration yaml

```sh
cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
iptables:
  masqueradeAll: true
clusterCIDR: "10.200.0.0/16"
EOF
```

Create the systemd unit file

```sh
cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

## Start the services

```sh
sudo systemctl daemon-reload
sudo systemctl enable containerd kubelet kube-proxy
sudo systemctl start containerd kubelet kube-proxy
```

## Verify 

SSH into any of the control plane instances, and try getting the nodes by executing

```sh
kubectl get nodes --kubeconfig admin.kubeconfig
```

A response displaying three nodes with a ready status means we're good to continue

```
NAME      STATUS   ROLES    AGE    VERSION
khw-wn0   Ready    <none>   5s   v1.29.0
khw-wn1   Ready    <none>   5s   v1.29.0
khw-wn2   Ready    <none>   5s   v1.29.0
```

## Configure remote access


We can now create the kubetcl configuration in our local environment, so we're able to connect to our cluster remotely

```sh

KUBERNETES_PUBLIC_ADDRESS=$(linode nodebalancers ls --format label,ipv4 --text --delimiter ' ' | grep khw | cut -f 2 -d ' ')


kubectl config set-cluster khw \
--certificate-authority=ca.pem \
--embed-certs=true \
--server=https://${KUBERNETES_PUBLIC_ADDRESS}:6443

kubectl config set-credentials admin \
--client-certificate=admin.pem \
--client-key=admin-key.pem

kubectl config set-context khw \
--cluster=khw \
--user=admin

```

Test the connection by executing:

```sh
kubectl config use-context khw
kubectl get nodes
```

If we get the same response, we can move to the next step

```
NAME      STATUS   ROLES    AGE    VERSION
khw-wn0   Ready    <none>   3m   v1.29.0
khw-wn1   Ready    <none>   3m   v1.29.0
khw-wn2   Ready    <none>   3m   v1.29.0
```

---

Next > [Configuring networking](6.networking.md)

