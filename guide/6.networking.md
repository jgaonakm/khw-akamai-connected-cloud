# Configuring networking

There are multiple ways to implement the Kubernetes networking model. In this lab we'll add the configuration required by the CNI plugins to work.

## Networking prerequisites

According to [the docs](https://kubernetes.io/docs/setup/production-environment/container-runtimes/#install-and-configure-prerequisites), we need to apply the following prerequisites on the worker nodes

```sh
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
```


> Disabled IPv4 packet forwarding on the worker nodes will prevent Pod networking from functioning. Hence, we need to ensure it's turned on.

Verify that system variables are set to 1

```sh
sysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward
```


## Creating the routes

Enable pods to communicate with other pods on different nodes by setting up routes matching the POD CIDR to the internal IP address.

khw-wn0
```sh
sudo ip route add 10.200.1.0/24 via 10.240.0.21
sudo ip route add 10.200.2.0/24 via 10.240.0.22 
```

khw-wn1
```sh
sudo ip route add 10.200.0.0/24 via 10.240.0.20               
sudo ip route add 10.200.2.0/24 via 10.240.0.22
```

khw-wn2
```sh
sudo ip route add 10.200.0.0/24 via 10.240.0.20               
sudo ip route add 10.200.1.0/24 via 10.240.0.21
```

## DNS Cluster Add-on

Our cluster is fully functional at this moment. We'll now deploy the [DNS add-on](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/), backed by [CoreDNS](https://coredns.io/), which provides DNS based service discovery to applications running inside the Kubernetes cluster.

We need the following objects

1. Service account
2. Cluster Role
3. Cluster Role binding
4. Config Map
5. Deployment
6. Service

You can create them using a single yml file, or as in this case, separate files to clearly identify what we're applying to our cluster.

Execute the following commands from a control plane instance:

```sh

cat <<EOF | kubectl apply  --kubeconfig admin.kubeconfig -f -
apiVersion: v1
kind: ServiceAccount
metadata:
  name: coredns
  namespace: kube-system
EOF
```

Result 

> serviceaccount/coredns created


```sh
cat <<EOF | kubectl apply  --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
rules:
  - apiGroups:
    - ""
    resources:
    - endpoints
    - services
    - pods
    - namespaces
    verbs:
    - list
    - watch
  - apiGroups:
    - discovery.k8s.io
    resources:
    - endpointslices
    verbs:
    - list
    - watch
EOF
```

Result:
> clusterrole.rbac.authorization.k8s.io/system:coredns created


```sh
cat <<EOF | kubectl apply  --kubeconfig admin.kubeconfig -f -
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  annotations:
    rbac.authorization.kubernetes.io/autoupdate: "true"
  labels:
    kubernetes.io/bootstrapping: rbac-defaults
  name: system:coredns
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:coredns
subjects:
- kind: ServiceAccount
  name: coredns
  namespace: kube-system
EOF
```
Result:
> clusterrolebinding.rbac.authorization.k8s.io/system:coredns created


```sh
cat <<EOF | kubectl apply  --kubeconfig admin.kubeconfig -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
          lameduck 5s
        }
        ready
        kubernetes cluster.local in-addr.arpa ip6.arpa {
           pods insecure
          fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf {
          max_concurrent 1000
        }
        cache 30
        loop
        reload
        loadbalance
    }
EOF
```
Result:
> configmap/coredns created

```sh
cat <<EOF | kubectl apply  --kubeconfig admin.kubeconfig -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: coredns
  namespace: kube-system
  labels:
    k8s-app: kube-dns
    kubernetes.io/name: "CoreDNS"
spec:
  replicas: 2
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  selector:
    matchLabels:
      k8s-app: kube-dns
  template:
    metadata:
      labels:
        k8s-app: kube-dns
    spec:
      priorityClassName: system-cluster-critical
      serviceAccountName: coredns
      tolerations:
        - key: "CriticalAddonsOnly"
          operator: "Exists"
      nodeSelector:
        kubernetes.io/os: linux
      affinity:
         podAntiAffinity:
           requiredDuringSchedulingIgnoredDuringExecution:
           - labelSelector:
               matchExpressions:
               - key: k8s-app
                 operator: In
                 values: ["kube-dns"]
             topologyKey: kubernetes.io/hostname
      containers:
      - name: coredns
        image: coredns/coredns:1.11.1
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            memory: 170Mi
          requests:
            cpu: 100m
            memory: 70Mi
        args: [ "-conf", "/etc/coredns/Corefile" ]
        volumeMounts:
        - name: config-volume
          mountPath: /etc/coredns
          readOnly: true
        ports:
        - containerPort: 53
          name: dns
          protocol: UDP
        - containerPort: 53
          name: dns-tcp
          protocol: TCP
        - containerPort: 9153
          name: metrics
          protocol: TCP
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            add:
            - NET_BIND_SERVICE
            drop:
            - all
          readOnlyRootFilesystem: true
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 60
          timeoutSeconds: 5
          successThreshold: 1
          failureThreshold: 5
        readinessProbe:
          httpGet:
            path: /ready
            port: 8181
            scheme: HTTP
          initialDelaySeconds: 30
          periodSeconds: 20
          failureThreshold: 3
          successThreshold: 1
          timeoutSeconds: 10
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: Default
      volumes:
        - name: config-volume
          configMap:
            name: coredns
            defaultMode: 420
            items:
            - key: Corefile
              path: Corefile
  progressDeadlineSeconds: 600
  revisionHistoryLimit: 10
EOF
```

Result:
> deployment.apps/coredns created

```sh

cat <<EOF | kubectl apply  --kubeconfig admin.kubeconfig -f -
apiVersion: v1
kind: Service
metadata:
  name: kube-dns
  namespace: kube-system
  annotations:
    prometheus.io/port: "9153"
    prometheus.io/scrape: "true"
  labels:
    k8s-app: kube-dns
    kubernetes.io/cluster-service: "true"
    kubernetes.io/name: "CoreDNS"
spec:
  selector:
    k8s-app: kube-dns
  clusterIP: 10.32.0.10
  ports:
  - name: dns
    port: 53
    protocol: UDP
  - name: dns-tcp
    port: 53
    protocol: TCP
  - name: metrics
    port: 9153
    protocol: TCP
EOF
```


Result:
> service/kube-dns created

Verify deployment succeded 

```sh
kubectl get events --sort-by=.metadata.creationTimestamp -n kube-system  --kubeconfig=admin.kubeconfig
```

Check that two pods are running
```sh
kubectl get pods -l k8s-app=kube-dns -n kube-system --kubeconfig=admin.kubeconfig
```

---

Next > [Smoke test](7.smokeTests.md)

